{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c9dbe3b",
   "metadata": {},
   "source": [
    "# Lecture 7: Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd927b8b",
   "metadata": {},
   "source": [
    "## First tokenize the entire short story (The Verdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d041c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters 20479\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "  raw_text = f.read()\n",
    "\n",
    "print(\"Total characters\", len(raw_text))\n",
    "raw_text[:99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "454d95b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'test!']\n"
     ]
    }
   ],
   "source": [
    "# Use regular expressions with re\n",
    "\n",
    "import re\n",
    "text = \"Hello, world. This is a test!\"\n",
    "result = re.split(r'(\\s)', text) # split where white spaces\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07b9f59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'test', '!', '']\n"
     ]
    }
   ],
   "source": [
    "# also split commas and periods\n",
    "result = re.split(r'([,.!]|\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8a4aff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ',', 'world', '.', 'This', 'is', 'a', 'test', '!']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()] # gets rid of whitespace\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b681838",
   "metadata": {},
   "source": [
    "Remove whitespace to reduce memory but can also have some defects where whitespace may have more meaning (i.e. in Python code).\n",
    "\n",
    "When building an LLM, think for applications whether it makes sense to remove whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116a8e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the final simple tokenization scheme\n",
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item for item in result if item.strip()]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cb0b5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preproccessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preproccessed = [item for item in preproccessed if item.strip()]\n",
    "print(preproccessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d62b4aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "print(len(preproccessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55c2c78",
   "metadata": {},
   "source": [
    "## Now we must convert tokens into Token IDs (step 2)\n",
    "\n",
    "*Tokens now have to have numerical representations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8799faa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1130"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words = sorted(set(preproccessed))\n",
    "vocab_size = len(all_words)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "997078db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the sorted vocab to a number (order)\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e834f8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n"
     ]
    }
   ],
   "source": [
    "for i, item in list(enumerate(vocab.items()))[:50]:\n",
    "  print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c369ba1",
   "metadata": {},
   "source": [
    "**Alphabetical order in the vocab** (see above)\n",
    "- Individual tokens\n",
    "- Individual integers\n",
    "\n",
    "We also need a decoder... map ID back to token (decode)\n",
    "\n",
    "Define two functions, _encode_ and __decode__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d80e189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "  def __init__(self, vocab):\n",
    "    self.str_to_int = vocab\n",
    "    self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "  def encode(self, input_text):\n",
    "    preproccessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', input_text)\n",
    "    preproccessed = [item.strip() for item in preproccessed if item.strip()]\n",
    "    ids = [self.str_to_int[s] for s in preproccessed]\n",
    "    return ids\n",
    "  \n",
    "  def decode(self, ids):\n",
    "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "    # add spaces back but not between punctuation!\n",
    "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502aa5f6",
   "metadata": {},
   "source": [
    "Take some text, encode it and decode it back! This is a simple sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5fe01327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizer(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know\"\n",
    "          Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87c85a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09de535c",
   "metadata": {},
   "source": [
    "Now encode something not in our vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ed064f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Where'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhere is my five iron?!!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(text)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(ids)\n",
      "Cell \u001b[0;32mIn[33], line 9\u001b[0m, in \u001b[0;36mSimpleTokenizer.encode\u001b[0;34m(self, input_text)\u001b[0m\n\u001b[1;32m      7\u001b[0m preproccessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, input_text)\n\u001b[1;32m      8\u001b[0m preproccessed \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preproccessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[0;32m----> 9\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_to_int[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preproccessed]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "Cell \u001b[0;32mIn[33], line 9\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m preproccessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, input_text)\n\u001b[1;32m      8\u001b[0m preproccessed \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preproccessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[0;32m----> 9\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_to_int[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preproccessed]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Where'"
     ]
    }
   ],
   "source": [
    "text = \"Where is my five iron?!!\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1afdd0",
   "metadata": {},
   "source": [
    "This shows an error message because this word is not in our vocab. This motivates __special context tokens...__\n",
    "- The tokenizer will handle unknown words\n",
    "- Unknown text token <|unk|>\n",
    "- Also add end of text token <|endoftext|>\n",
    "\n",
    "Add these two!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ec08fe35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add two more tokens to our vocab\n",
    "import enum\n",
    "\n",
    "\n",
    "all_tokens = sorted(list(set(preproccessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\",\"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "len(vocab) # verify we added two tokens bc 1130 before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5f4fcbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "# show these two tokens were actually added\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "  print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6e134fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer2:\n",
    "  def __init__(self, vocab):\n",
    "    self.str_to_int = vocab\n",
    "    self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "  def encode(self, input_text):\n",
    "    preproccessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', input_text)\n",
    "    preproccessed = [item.strip() for item in preproccessed if item.strip()]\n",
    "    preproccessed = [\n",
    "      (item if item in self.str_to_int\n",
    "      else \"<|unk|>\") for item in preproccessed\n",
    "    ]\n",
    "    ids = [self.str_to_int[s] for s in preproccessed]\n",
    "    return ids\n",
    "  \n",
    "  def decode(self, ids):\n",
    "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "    # add spaces back but not between punctuation!\n",
    "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cda10290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where is my five iron?!! <|endoftext|> Hey there, do you like tea?\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizer2(vocab)\n",
    "\n",
    "text1 = \"Where is my five iron?!!\"\n",
    "text2 = \"Hey there, do you like tea?\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "83778635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131,\n",
       " 584,\n",
       " 697,\n",
       " 445,\n",
       " 1131,\n",
       " 10,\n",
       " 0,\n",
       " 0,\n",
       " 1130,\n",
       " 1131,\n",
       " 992,\n",
       " 5,\n",
       " 355,\n",
       " 1126,\n",
       " 628,\n",
       " 975,\n",
       " 10]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a7a3bef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|> is my five <|unk|>?!! <|endoftext|> <|unk|> there, do you like tea?'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f316a00",
   "metadata": {},
   "source": [
    "_We can restore where the unknown tokens are without failure_\n",
    "\n",
    "The original text does not inlcude 'Where,' 'hey,' 'iron' but no error is formed\n",
    "\n",
    "__Some more tokens:__\n",
    "- BOS (beginning of sequence)\n",
    "- EOS (end of sequence)\n",
    "- PAD (padding)\n",
    "\n",
    "GPT will ONLY use <|endoftext|>\n",
    "\n",
    "GPT also does byte-pair encoding (no <|unk|>) so it breaks words down into sub-units (so worst case just the individual characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d79683d",
   "metadata": {},
   "source": [
    "# Lecture 8: Byte-pair encoding\n",
    "\n",
    "Last time, we did a very simple tokenizer, but GPT actually does byte-pair which we will start now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab7f178",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
