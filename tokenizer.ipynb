{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c9dbe3b",
   "metadata": {},
   "source": [
    "# Lecture 7: Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd927b8b",
   "metadata": {},
   "source": [
    "## First tokenize the entire short story (The Verdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d041c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters 20479\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "  raw_text = f.read()\n",
    "\n",
    "print(\"Total characters\", len(raw_text))\n",
    "raw_text[:99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "454d95b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'test!']\n"
     ]
    }
   ],
   "source": [
    "# Use regular expressions with re\n",
    "\n",
    "import re\n",
    "text = \"Hello, world. This is a test!\"\n",
    "result = re.split(r'(\\s)', text) # split where white spaces\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07b9f59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'test', '!', '']\n"
     ]
    }
   ],
   "source": [
    "# also split commas and periods\n",
    "result = re.split(r'([,.!]|\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8a4aff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ',', 'world', '.', 'This', 'is', 'a', 'test', '!']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()] # gets rid of whitespace\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b681838",
   "metadata": {},
   "source": [
    "Remove whitespace to reduce memory but can also have some defects where whitespace may have more meaning (i.e. in Python code).\n",
    "\n",
    "When building an LLM, think for applications whether it makes sense to remove whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "116a8e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the final simple tokenization scheme\n",
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item for item in result if item.strip()]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3cb0b5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preproccessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preproccessed = [item for item in preproccessed if item.strip()]\n",
    "print(preproccessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d62b4aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "print(len(preproccessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55c2c78",
   "metadata": {},
   "source": [
    "## Now we must convert tokens into Token IDs (step 2)\n",
    "\n",
    "*Tokens now have to have numerical representations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8799faa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1130"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words = sorted(set(preproccessed))\n",
    "vocab_size = len(all_words)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "997078db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the sorted vocab to a number (order)\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e834f8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n"
     ]
    }
   ],
   "source": [
    "for i, item in list(enumerate(vocab.items()))[:50]:\n",
    "  print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c369ba1",
   "metadata": {},
   "source": [
    "**Alphabetical order in the vocab** (see above)\n",
    "- Individual tokens\n",
    "- Individual integers\n",
    "\n",
    "We also need a decoder... map ID back to token (decode)\n",
    "\n",
    "Define two functions, _encode_ and __decode__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d80e189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "  def __init__(self, vocab):\n",
    "    self.str_to_int = vocab\n",
    "    self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "  def encode(self, input_text):\n",
    "    preproccessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', input_text)\n",
    "    preproccessed = [item.strip() for item in preproccessed if item.strip()]\n",
    "    ids = [self.str_to_int[s] for s in preproccessed]\n",
    "    return ids\n",
    "  \n",
    "  def decode(self, ids):\n",
    "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "    # add spaces back but not between punctuation!\n",
    "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502aa5f6",
   "metadata": {},
   "source": [
    "Take some text, encode it and decode it back! This is a simple sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5fe01327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizer(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know\"\n",
    "          Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "87c85a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09de535c",
   "metadata": {},
   "source": [
    "Now encode something not in our vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ed064f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Where'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhere is my five iron?!!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(text)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(ids)\n",
      "Cell \u001b[0;32mIn[33], line 9\u001b[0m, in \u001b[0;36mSimpleTokenizer.encode\u001b[0;34m(self, input_text)\u001b[0m\n\u001b[1;32m      7\u001b[0m preproccessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, input_text)\n\u001b[1;32m      8\u001b[0m preproccessed \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preproccessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[0;32m----> 9\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_to_int[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preproccessed]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "Cell \u001b[0;32mIn[33], line 9\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m preproccessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, input_text)\n\u001b[1;32m      8\u001b[0m preproccessed \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preproccessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[0;32m----> 9\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_to_int[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preproccessed]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Where'"
     ]
    }
   ],
   "source": [
    "text = \"Where is my five iron?!!\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1afdd0",
   "metadata": {},
   "source": [
    "This shows an error message because this word is not in our vocab. This motivates __special context tokens...__\n",
    "- The tokenizer will handle unknown words\n",
    "- Unknown text token <|unk|>\n",
    "- Also add end of text token <|endoftext|>\n",
    "\n",
    "Add these two!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec08fe35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add two more tokens to our vocab\n",
    "import enum\n",
    "\n",
    "\n",
    "all_tokens = sorted(list(set(preproccessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\",\"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "len(vocab) # verify we added two tokens bc 1130 before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4fcbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "# show these two tokens were actually added\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "  print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e134fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer2:\n",
    "  def __init__(self, vocab):\n",
    "    self.str_to_int = vocab\n",
    "    self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "  def encode(self, input_text):\n",
    "    preproccessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', input_text)\n",
    "    preproccessed = [item.strip() for item in preproccessed if item.strip()]\n",
    "    preproccessed = [\n",
    "      (item if item in self.str_to_int\n",
    "      else \"<|unk|>\") for item in preproccessed\n",
    "    ]\n",
    "    ids = [self.str_to_int[s] for s in preproccessed]\n",
    "    return ids\n",
    "  \n",
    "  def decode(self, ids):\n",
    "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "    # add spaces back but not between punctuation!\n",
    "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda10290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where is my five iron?!! <|endoftext|> Hey there, do you like tea?\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizer2(vocab)\n",
    "\n",
    "text1 = \"Where is my five iron?!!\"\n",
    "text2 = \"Hey there, do you like tea?\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83778635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131,\n",
       " 584,\n",
       " 697,\n",
       " 445,\n",
       " 1131,\n",
       " 10,\n",
       " 0,\n",
       " 0,\n",
       " 1130,\n",
       " 1131,\n",
       " 992,\n",
       " 5,\n",
       " 355,\n",
       " 1126,\n",
       " 628,\n",
       " 975,\n",
       " 10]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a3bef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|> is my five <|unk|>?!! <|endoftext|> <|unk|> there, do you like tea?'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f316a00",
   "metadata": {},
   "source": [
    "_We can restore where the unknown tokens are without failure_\n",
    "\n",
    "The original text does not inlcude 'Where,' 'hey,' 'iron' but no error is formed\n",
    "\n",
    "__Some more tokens:__\n",
    "- BOS (beginning of sequence)\n",
    "- EOS (end of sequence)\n",
    "- PAD (padding)\n",
    "\n",
    "GPT will ONLY use <|endoftext|>\n",
    "\n",
    "GPT also does byte-pair encoding (no <|unk|>) so it breaks words down into sub-units (so worst case just the individual characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d79683d",
   "metadata": {},
   "source": [
    "# Lecture 8: Byte-pair encoding\n",
    "\n",
    "Last time, we did a very simple tokenizer, but GPT actually does byte-pair which we will start now!\n",
    "\n",
    "tiktoken allows the use of OpenAIs GPT2 BPE tokenizer to use their vocabulary on a large corpus to tokenize any text. _I understand how BPE works (I've done it by hand), but it makes sense to use the pre-developed tokenizer_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aab7f178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /Users/ethanhersch/anaconda3/lib/python3.11/site-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/ethanhersch/anaconda3/lib/python3.11/site-packages (from tiktoken) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/ethanhersch/anaconda3/lib/python3.11/site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/ethanhersch/anaconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ethanhersch/anaconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ethanhersch/anaconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ethanhersch/anaconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2026.1.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# first install the byte-pair tokenizer\n",
    "\n",
    "! pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "51f4219f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "from importlib import metadata\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d11b86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff25ba37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "  \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "  \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa3cbda",
   "metadata": {},
   "source": [
    "This tokenizer __works__ with unknown tokens because goes to character level!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52496f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64, 28233, 26502, 220, 959, 288, 459, 289, 44452]\n",
      "aekerwon ier dast huden\n"
     ]
    }
   ],
   "source": [
    "integers = tokenizer.encode(\"aekerwon ier dast huden\")\n",
    "print(integers)\n",
    "\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42528350",
   "metadata": {},
   "source": [
    "# Lecture 9: Input-Target Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3511ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5145"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_text = tokenizer.encode(raw_text)\n",
    "len(enc_text) # our vocab has a length 5145 (5145 mappings of tokens to IDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa8b300",
   "metadata": {},
   "source": [
    "Let's demo with the first 50 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0d4960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40,\n",
       " 367,\n",
       " 2885,\n",
       " 1464,\n",
       " 1807,\n",
       " 3619,\n",
       " 402,\n",
       " 271,\n",
       " 10899,\n",
       " 2138,\n",
       " 257,\n",
       " 7026,\n",
       " 15632,\n",
       " 438,\n",
       " 2016,\n",
       " 257,\n",
       " 922,\n",
       " 5891,\n",
       " 1576,\n",
       " 438,\n",
       " 568,\n",
       " 340,\n",
       " 373,\n",
       " 645,\n",
       " 1049,\n",
       " 5975,\n",
       " 284,\n",
       " 502,\n",
       " 284,\n",
       " 3285,\n",
       " 326,\n",
       " 11,\n",
       " 287,\n",
       " 262,\n",
       " 6001,\n",
       " 286,\n",
       " 465,\n",
       " 13476,\n",
       " 11,\n",
       " 339,\n",
       " 550,\n",
       " 5710,\n",
       " 465,\n",
       " 12036,\n",
       " 11,\n",
       " 6405,\n",
       " 257,\n",
       " 5527,\n",
       " 27075,\n",
       " 11]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = enc_text[:50]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b186f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairs(text):\n",
    "  x = text[:-1]\n",
    "  y = text[1:]\n",
    "  print(x)\n",
    "  print(y)\n",
    "  print(len(x))\n",
    "  print(len(y))\n",
    "\n",
    "  input_target_pairs = {}\n",
    "  for i in range(len(x)):\n",
    "    input_target_pairs[tuple(x[:i+1])] = y[i]\n",
    "\n",
    "  return input_target_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa232f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138, 257, 7026, 15632, 438, 2016, 257, 922, 5891, 1576, 438, 568, 340, 373, 645, 1049, 5975, 284, 502, 284, 3285, 326, 11, 287, 262, 6001, 286, 465, 13476, 11, 339, 550, 5710, 465, 12036, 11, 6405, 257, 5527, 27075]\n",
      "[367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138, 257, 7026, 15632, 438, 2016, 257, 922, 5891, 1576, 438, 568, 340, 373, 645, 1049, 5975, 284, 502, 284, 3285, 326, 11, 287, 262, 6001, 286, 465, 13476, 11, 339, 550, 5710, 465, 12036, 11, 6405, 257, 5527, 27075, 11]\n",
      "49\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "input_target_pairs = create_pairs(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d736d16d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(40,): 367,\n",
       " (40, 367): 2885,\n",
       " (40, 367, 2885): 1464,\n",
       " (40, 367, 2885, 1464): 1807,\n",
       " (40, 367, 2885, 1464, 1807): 3619,\n",
       " (40, 367, 2885, 1464, 1807, 3619): 402,\n",
       " (40, 367, 2885, 1464, 1807, 3619, 402): 271,\n",
       " (40, 367, 2885, 1464, 1807, 3619, 402, 271): 10899,\n",
       " (40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899): 2138,\n",
       " (40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138): 257,\n",
       " (40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138, 257): 7026,\n",
       " (40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138, 257, 7026): 15632,\n",
       " (40,\n",
       "  367,\n",
       "  2885,\n",
       "  1464,\n",
       "  1807,\n",
       "  3619,\n",
       "  402,\n",
       "  271,\n",
       "  10899,\n",
       "  2138,\n",
       "  257,\n",
       "  7026,\n",
       "  15632): 438,\n",
       " (40,\n",
       "  367,\n",
       "  2885,\n",
       "  1464,\n",
       "  1807,\n",
       "  3619,\n",
       "  402,\n",
       "  271,\n",
       "  10899,\n",
       "  2138,\n",
       "  257,\n",
       "  7026,\n",
       "  15632,\n",
       "  438): 2016,\n",
       " (40,\n",
       "  367,\n",
       "  2885,\n",
       "  1464,\n",
       "  1807,\n",
       "  3619,\n",
       "  402,\n",
       "  271,\n",
       "  10899,\n",
       "  2138,\n",
       "  257,\n",
       "  7026,\n",
       "  15632,\n",
       "  438,\n",
       "  2016): 257,\n",
       " (40,\n",
       "  367,\n",
       "  2885,\n",
       "  1464,\n",
       "  1807,\n",
       "  3619,\n",
       "  402,\n",
       "  271,\n",
       "  10899,\n",
       "  2138,\n",
       "  257,\n",
       "  7026,\n",
       "  15632,\n",
       "  438,\n",
       "  2016,\n",
       "  257): 922,\n",
       " (40,\n",
       "  367,\n",
       "  2885,\n",
       "  1464,\n",
       "  1807,\n",
       "  3619,\n",
       "  402,\n",
       "  271,\n",
       "  10899,\n",
       "  2138,\n",
       "  257,\n",
       "  7026,\n",
       "  15632,\n",
       "  438,\n",
       "  2016,\n",
       "  257,\n",
       "  922): 5891,\n",
       " (40,\n",
       "  367,\n",
       "  2885,\n",
       "  1464,\n",
       "  1807,\n",
       "  3619,\n",
       "  402,\n",
       "  271,\n",
       "  10899,\n",
       "  2138,\n",
       "  257,\n",
       "  7026,\n",
       "  15632,\n",
       "  438,\n",
       "  2016,\n",
       "  257,\n",
       "  922,\n",
       "  5891): 1576,\n",
       " (40,\n",
       "  367,\n",
       "  2885,\n",
       "  1464,\n",
       "  1807,\n",
       "  3619,\n",
       "  402,\n",
       "  271,\n",
       "  10899,\n",
       "  2138,\n",
       "  257,\n",
       "  7026,\n",
       "  15632,\n",
       "  438,\n",
       "  2016,\n",
       "  257,\n",
       "  922,\n",
       "  5891,\n",
       "  1576): 438,\n",
       " (40,\n",
       "  367,\n",
       "  2885,\n",
       "  1464,\n",
       "  1807,\n",
       "  3619,\n",
       "  402,\n",
       "  271,\n",
       "  10899,\n",
       "  2138,\n",
       "  257,\n",
       "  7026,\n",
       "  15632,\n",
       "  438,\n",
       "  2016,\n",
       "  257,\n",
       "  922,\n",
       "  5891,\n",
       "  1576,\n",
       "  438): 568,\n",
       " (40,\n",
       "  367,\n",
       "  2885,\n",
       "  1464,\n",
       "  1807,\n",
       "  3619,\n",
       "  402,\n",
       "  271,\n",
       "  10899,\n",
       "  2138,\n",
       "  257,\n",
       "  7026,\n",
       "  15632,\n",
       "  438,\n",
       "  2016,\n",
       "  257,\n",
       "  922,\n",
       "  5891,\n",
       "  1576,\n",
       "  438,\n",
       "  568): 340,\n",
       " (40,\n",
       "  367,\n",
       "  2885,\n",
       "  1464,\n",
       "  1807,\n",
       "  3619,\n",
       "  402,\n",
       "  271,\n",
       "  10899,\n",
       "  2138,\n",
       "  257,\n",
       "  7026,\n",
       "  15632,\n",
       "  438,\n",
       "  2016,\n",
       "  257,\n",
       "  922,\n",
       "  5891,\n",
       "  1576,\n",
       "  438,\n",
       "  568,\n",
       "  340): 373,\n",
       " (40,\n",
       "  367,\n",
       "  2885,\n",
       "  1464,\n",
       "  1807,\n",
       "  3619,\n",
       "  402,\n",
       "  271,\n",
       "  10899,\n",
       "  2138,\n",
       "  257,\n",
       "  7026,\n",
       "  15632,\n",
       "  438,\n",
       "  2016,\n",
       "  257,\n",
       "  922,\n",
       "  5891,\n",
       "  1576,\n",
       "  438,\n",
       "  568,\n",
       "  340,\n",
       "  373): 645,\n",
       " (40,\n",
       "  367,\n",
       "  2885,\n",
       "  1464,\n",
       "  1807,\n",
       "  3619,\n",
       "  402,\n",
       "  271,\n",
       "  10899,\n",
       "  2138,\n",
       "  257,\n",
       "  7026,\n",
       "  15632,\n",
       "  438,\n",
       "  2016,\n",
       "  257,\n",
       "  922,\n",
       "  5891,\n",
       "  1576,\n",
       "  438,\n",
       "  568,\n",
       "  340,\n",
       "  373,\n",
       "  645): 1049,\n",
       " (40,\n",
       "  367,\n",
       "  2885,\n",
       "  1464,\n",
       "  1807,\n",
       "  3619,\n",
       "  402,\n",
       "  271,\n",
       "  10899,\n",
       "  2138,\n",
       "  257,\n",
       "  7026,\n",
       "  15632,\n",
       "  438,\n",
       "  2016,\n",
       "  257,\n",
       "  922,\n",
       "  5891,\n",
       "  1576,\n",
       "  438,\n",
       "  568,\n",
       "  340,\n",
       "  373,\n",
       "  645,\n",
       "  1049): 5975,\n",
       " (40,\n",
       "  367,\n",
       "  2885,\n",
       "  1464,\n",
       "  1807,\n",
       "  3619,\n",
       "  402,\n",
       "  271,\n",
       "  10899,\n",
       "  2138,\n",
       "  257,\n",
       "  7026,\n",
       "  15632,\n",
       "  438,\n",
       "  2016,\n",
       "  257,\n",
       "  922,\n",
       "  5891,\n",
       "  1576,\n",
       "  438,\n",
       "  568,\n",
       "  340,\n",
       "  373,\n",
       "  645,\n",
       "  1049,\n",
       "  5975): 284,\n",
       " (40,\n",
       "  367,\n",
       "  2885,\n",
       "  1464,\n",
       "  1807,\n",
       "  3619,\n",
       "  402,\n",
       "  271,\n",
       "  10899,\n",
       "  2138,\n",
       "  257,\n",
       "  7026,\n",
       "  15632,\n",
       "  438,\n",
       "  2016,\n",
       "  257,\n",
       "  922,\n",
       "  5891,\n",
       "  1576,\n",
       "  438,\n",
       "  568,\n",
       "  340,\n",
       "  373,\n",
       "  645,\n",
       "  1049,\n",
       "  5975,\n",
       "  284): 502,\n",
       " (40,\n",
       "  367,\n",
       "  2885,\n",
       "  1464,\n",
       "  1807,\n",
       "  3619,\n",
       "  402,\n",
       "  271,\n",
       "  10899,\n",
       "  2138,\n",
       "  257,\n",
       "  7026,\n",
       "  15632,\n",
       "  438,\n",
       "  2016,\n",
       "  257,\n",
       "  922,\n",
       "  5891,\n",
       "  1576,\n",
       "  438,\n",
       "  568,\n",
       "  340,\n",
       "  373,\n",
       "  645,\n",
       "  1049,\n",
       "  5975,\n",
       "  284,\n",
       "  502): 284,\n",
       " (40,\n",
       "  367,\n",
       "  2885,\n",
       "  1464,\n",
       "  1807,\n",
       "  3619,\n",
       "  402,\n",
       "  271,\n",
       "  10899,\n",
       "  2138,\n",
       "  257,\n",
       "  7026,\n",
       "  15632,\n",
       "  438,\n",
       "  2016,\n",
       "  257,\n",
       "  922,\n",
       "  5891,\n",
       "  1576,\n",
       "  438,\n",
       "  568,\n",
       "  340,\n",
       "  373,\n",
       "  645,\n",
       "  1049,\n",
       "  5975,\n",
       "  284,\n",
       "  502,\n",
       "  284): 3285,\n",
       " (40,\n",
       "  367,\n",
       "  2885,\n",
       "  1464,\n",
       "  1807,\n",
       "  3619,\n",
       "  402,\n",
       "  271,\n",
       "  10899,\n",
       "  2138,\n",
       "  257,\n",
       "  7026,\n",
       "  15632,\n",
       "  438,\n",
       "  2016,\n",
       "  257,\n",
       "  922,\n",
       "  5891,\n",
       "  1576,\n",
       "  438,\n",
       "  568,\n",
       "  340,\n",
       "  373,\n",
       "  645,\n",
       "  1049,\n",
       "  5975,\n",
       "  284,\n",
       "  502,\n",
       "  284,\n",
       "  3285): 326,\n",
       " (40,\n",
       "  367,\n",
       "  2885,\n",
       "  1464,\n",
       "  1807,\n",
       "  3619,\n",
       "  402,\n",
       "  271,\n",
       "  10899,\n",
       "  2138,\n",
       "  257,\n",
       "  7026,\n",
       "  15632,\n",
       "  438,\n",
       "  2016,\n",
       "  257,\n",
       "  922,\n",
       "  5891,\n",
       "  1576,\n",
       "  438,\n",
       "  568,\n",
       "  340,\n",
       "  373,\n",
       "  645,\n",
       "  1049,\n",
       "  5975,\n",
       "  284,\n",
       "  502,\n",
       "  284,\n",
       "  3285,\n",
       "  326): 11,\n",
       " (40,\n",
       "  367,\n",
       "  2885,\n",
       "  1464,\n",
       "  1807,\n",
       "  3619,\n",
       "  402,\n",
       "  271,\n",
       "  10899,\n",
       "  2138,\n",
       "  257,\n",
       "  7026,\n",
       "  15632,\n",
       "  438,\n",
       "  2016,\n",
       "  257,\n",
       "  922,\n",
       "  5891,\n",
       "  1576,\n",
       "  438,\n",
       "  568,\n",
       "  340,\n",
       "  373,\n",
       "  645,\n",
       "  1049,\n",
       "  5975,\n",
       "  284,\n",
       "  502,\n",
       "  284,\n",
       "  3285,\n",
       "  326,\n",
       "  11): 287,\n",
       " (40,\n",
       "  367,\n",
       "  2885,\n",
       "  1464,\n",
       "  1807,\n",
       "  3619,\n",
       "  402,\n",
       "  271,\n",
       "  10899,\n",
       "  2138,\n",
       "  257,\n",
       "  7026,\n",
       "  15632,\n",
       "  438,\n",
       "  2016,\n",
       "  257,\n",
       "  922,\n",
       "  5891,\n",
       "  1576,\n",
       "  438,\n",
       "  568,\n",
       "  340,\n",
       "  373,\n",
       "  645,\n",
       "  1049,\n",
       "  5975,\n",
       "  284,\n",
       "  502,\n",
       "  284,\n",
       "  3285,\n",
       "  326,\n",
       "  11,\n",
       "  287): 262,\n",
       " (40,\n",
       "  367,\n",
       "  2885,\n",
       "  1464,\n",
       "  1807,\n",
       "  3619,\n",
       "  402,\n",
       "  271,\n",
       "  10899,\n",
       "  2138,\n",
       "  257,\n",
       "  7026,\n",
       "  15632,\n",
       "  438,\n",
       "  2016,\n",
       "  257,\n",
       "  922,\n",
       "  5891,\n",
       "  1576,\n",
       "  438,\n",
       "  568,\n",
       "  340,\n",
       "  373,\n",
       "  645,\n",
       "  1049,\n",
       "  5975,\n",
       "  284,\n",
       "  502,\n",
       "  284,\n",
       "  3285,\n",
       "  326,\n",
       "  11,\n",
       "  287,\n",
       "  262): 6001,\n",
       " (40,\n",
       "  367,\n",
       "  2885,\n",
       "  1464,\n",
       "  1807,\n",
       "  3619,\n",
       "  402,\n",
       "  271,\n",
       "  10899,\n",
       "  2138,\n",
       "  257,\n",
       "  7026,\n",
       "  15632,\n",
       "  438,\n",
       "  2016,\n",
       "  257,\n",
       "  922,\n",
       "  5891,\n",
       "  1576,\n",
       "  438,\n",
       "  568,\n",
       "  340,\n",
       "  373,\n",
       "  645,\n",
       "  1049,\n",
       "  5975,\n",
       "  284,\n",
       "  502,\n",
       "  284,\n",
       "  3285,\n",
       "  326,\n",
       "  11,\n",
       "  287,\n",
       "  262,\n",
       "  6001): 286,\n",
       " (40,\n",
       "  367,\n",
       "  2885,\n",
       "  1464,\n",
       "  1807,\n",
       "  3619,\n",
       "  402,\n",
       "  271,\n",
       "  10899,\n",
       "  2138,\n",
       "  257,\n",
       "  7026,\n",
       "  15632,\n",
       "  438,\n",
       "  2016,\n",
       "  257,\n",
       "  922,\n",
       "  5891,\n",
       "  1576,\n",
       "  438,\n",
       "  568,\n",
       "  340,\n",
       "  373,\n",
       "  645,\n",
       "  1049,\n",
       "  5975,\n",
       "  284,\n",
       "  502,\n",
       "  284,\n",
       "  3285,\n",
       "  326,\n",
       "  11,\n",
       "  287,\n",
       "  262,\n",
       "  6001,\n",
       "  286): 465,\n",
       " (40,\n",
       "  367,\n",
       "  2885,\n",
       "  1464,\n",
       "  1807,\n",
       "  3619,\n",
       "  402,\n",
       "  271,\n",
       "  10899,\n",
       "  2138,\n",
       "  257,\n",
       "  7026,\n",
       "  15632,\n",
       "  438,\n",
       "  2016,\n",
       "  257,\n",
       "  922,\n",
       "  5891,\n",
       "  1576,\n",
       "  438,\n",
       "  568,\n",
       "  340,\n",
       "  373,\n",
       "  645,\n",
       "  1049,\n",
       "  5975,\n",
       "  284,\n",
       "  502,\n",
       "  284,\n",
       "  3285,\n",
       "  326,\n",
       "  11,\n",
       "  287,\n",
       "  262,\n",
       "  6001,\n",
       "  286,\n",
       "  465): 13476,\n",
       " (40,\n",
       "  367,\n",
       "  2885,\n",
       "  1464,\n",
       "  1807,\n",
       "  3619,\n",
       "  402,\n",
       "  271,\n",
       "  10899,\n",
       "  2138,\n",
       "  257,\n",
       "  7026,\n",
       "  15632,\n",
       "  438,\n",
       "  2016,\n",
       "  257,\n",
       "  922,\n",
       "  5891,\n",
       "  1576,\n",
       "  438,\n",
       "  568,\n",
       "  340,\n",
       "  373,\n",
       "  645,\n",
       "  1049,\n",
       "  5975,\n",
       "  284,\n",
       "  502,\n",
       "  284,\n",
       "  3285,\n",
       "  326,\n",
       "  11,\n",
       "  287,\n",
       "  262,\n",
       "  6001,\n",
       "  286,\n",
       "  465,\n",
       "  13476): 11,\n",
       " (40,\n",
       "  367,\n",
       "  2885,\n",
       "  1464,\n",
       "  1807,\n",
       "  3619,\n",
       "  402,\n",
       "  271,\n",
       "  10899,\n",
       "  2138,\n",
       "  257,\n",
       "  7026,\n",
       "  15632,\n",
       "  438,\n",
       "  2016,\n",
       "  257,\n",
       "  922,\n",
       "  5891,\n",
       "  1576,\n",
       "  438,\n",
       "  568,\n",
       "  340,\n",
       "  373,\n",
       "  645,\n",
       "  1049,\n",
       "  5975,\n",
       "  284,\n",
       "  502,\n",
       "  284,\n",
       "  3285,\n",
       "  326,\n",
       "  11,\n",
       "  287,\n",
       "  262,\n",
       "  6001,\n",
       "  286,\n",
       "  465,\n",
       "  13476,\n",
       "  11): 339,\n",
       " (40,\n",
       "  367,\n",
       "  2885,\n",
       "  1464,\n",
       "  1807,\n",
       "  3619,\n",
       "  402,\n",
       "  271,\n",
       "  10899,\n",
       "  2138,\n",
       "  257,\n",
       "  7026,\n",
       "  15632,\n",
       "  438,\n",
       "  2016,\n",
       "  257,\n",
       "  922,\n",
       "  5891,\n",
       "  1576,\n",
       "  438,\n",
       "  568,\n",
       "  340,\n",
       "  373,\n",
       "  645,\n",
       "  1049,\n",
       "  5975,\n",
       "  284,\n",
       "  502,\n",
       "  284,\n",
       "  3285,\n",
       "  326,\n",
       "  11,\n",
       "  287,\n",
       "  262,\n",
       "  6001,\n",
       "  286,\n",
       "  465,\n",
       "  13476,\n",
       "  11,\n",
       "  339): 550,\n",
       " (40,\n",
       "  367,\n",
       "  2885,\n",
       "  1464,\n",
       "  1807,\n",
       "  3619,\n",
       "  402,\n",
       "  271,\n",
       "  10899,\n",
       "  2138,\n",
       "  257,\n",
       "  7026,\n",
       "  15632,\n",
       "  438,\n",
       "  2016,\n",
       "  257,\n",
       "  922,\n",
       "  5891,\n",
       "  1576,\n",
       "  438,\n",
       "  568,\n",
       "  340,\n",
       "  373,\n",
       "  645,\n",
       "  1049,\n",
       "  5975,\n",
       "  284,\n",
       "  502,\n",
       "  284,\n",
       "  3285,\n",
       "  326,\n",
       "  11,\n",
       "  287,\n",
       "  262,\n",
       "  6001,\n",
       "  286,\n",
       "  465,\n",
       "  13476,\n",
       "  11,\n",
       "  339,\n",
       "  550): 5710,\n",
       " (40,\n",
       "  367,\n",
       "  2885,\n",
       "  1464,\n",
       "  1807,\n",
       "  3619,\n",
       "  402,\n",
       "  271,\n",
       "  10899,\n",
       "  2138,\n",
       "  257,\n",
       "  7026,\n",
       "  15632,\n",
       "  438,\n",
       "  2016,\n",
       "  257,\n",
       "  922,\n",
       "  5891,\n",
       "  1576,\n",
       "  438,\n",
       "  568,\n",
       "  340,\n",
       "  373,\n",
       "  645,\n",
       "  1049,\n",
       "  5975,\n",
       "  284,\n",
       "  502,\n",
       "  284,\n",
       "  3285,\n",
       "  326,\n",
       "  11,\n",
       "  287,\n",
       "  262,\n",
       "  6001,\n",
       "  286,\n",
       "  465,\n",
       "  13476,\n",
       "  11,\n",
       "  339,\n",
       "  550,\n",
       "  5710): 465,\n",
       " (40,\n",
       "  367,\n",
       "  2885,\n",
       "  1464,\n",
       "  1807,\n",
       "  3619,\n",
       "  402,\n",
       "  271,\n",
       "  10899,\n",
       "  2138,\n",
       "  257,\n",
       "  7026,\n",
       "  15632,\n",
       "  438,\n",
       "  2016,\n",
       "  257,\n",
       "  922,\n",
       "  5891,\n",
       "  1576,\n",
       "  438,\n",
       "  568,\n",
       "  340,\n",
       "  373,\n",
       "  645,\n",
       "  1049,\n",
       "  5975,\n",
       "  284,\n",
       "  502,\n",
       "  284,\n",
       "  3285,\n",
       "  326,\n",
       "  11,\n",
       "  287,\n",
       "  262,\n",
       "  6001,\n",
       "  286,\n",
       "  465,\n",
       "  13476,\n",
       "  11,\n",
       "  339,\n",
       "  550,\n",
       "  5710,\n",
       "  465): 12036,\n",
       " (40,\n",
       "  367,\n",
       "  2885,\n",
       "  1464,\n",
       "  1807,\n",
       "  3619,\n",
       "  402,\n",
       "  271,\n",
       "  10899,\n",
       "  2138,\n",
       "  257,\n",
       "  7026,\n",
       "  15632,\n",
       "  438,\n",
       "  2016,\n",
       "  257,\n",
       "  922,\n",
       "  5891,\n",
       "  1576,\n",
       "  438,\n",
       "  568,\n",
       "  340,\n",
       "  373,\n",
       "  645,\n",
       "  1049,\n",
       "  5975,\n",
       "  284,\n",
       "  502,\n",
       "  284,\n",
       "  3285,\n",
       "  326,\n",
       "  11,\n",
       "  287,\n",
       "  262,\n",
       "  6001,\n",
       "  286,\n",
       "  465,\n",
       "  13476,\n",
       "  11,\n",
       "  339,\n",
       "  550,\n",
       "  5710,\n",
       "  465,\n",
       "  12036): 11,\n",
       " (40,\n",
       "  367,\n",
       "  2885,\n",
       "  1464,\n",
       "  1807,\n",
       "  3619,\n",
       "  402,\n",
       "  271,\n",
       "  10899,\n",
       "  2138,\n",
       "  257,\n",
       "  7026,\n",
       "  15632,\n",
       "  438,\n",
       "  2016,\n",
       "  257,\n",
       "  922,\n",
       "  5891,\n",
       "  1576,\n",
       "  438,\n",
       "  568,\n",
       "  340,\n",
       "  373,\n",
       "  645,\n",
       "  1049,\n",
       "  5975,\n",
       "  284,\n",
       "  502,\n",
       "  284,\n",
       "  3285,\n",
       "  326,\n",
       "  11,\n",
       "  287,\n",
       "  262,\n",
       "  6001,\n",
       "  286,\n",
       "  465,\n",
       "  13476,\n",
       "  11,\n",
       "  339,\n",
       "  550,\n",
       "  5710,\n",
       "  465,\n",
       "  12036,\n",
       "  11): 6405,\n",
       " (40,\n",
       "  367,\n",
       "  2885,\n",
       "  1464,\n",
       "  1807,\n",
       "  3619,\n",
       "  402,\n",
       "  271,\n",
       "  10899,\n",
       "  2138,\n",
       "  257,\n",
       "  7026,\n",
       "  15632,\n",
       "  438,\n",
       "  2016,\n",
       "  257,\n",
       "  922,\n",
       "  5891,\n",
       "  1576,\n",
       "  438,\n",
       "  568,\n",
       "  340,\n",
       "  373,\n",
       "  645,\n",
       "  1049,\n",
       "  5975,\n",
       "  284,\n",
       "  502,\n",
       "  284,\n",
       "  3285,\n",
       "  326,\n",
       "  11,\n",
       "  287,\n",
       "  262,\n",
       "  6001,\n",
       "  286,\n",
       "  465,\n",
       "  13476,\n",
       "  11,\n",
       "  339,\n",
       "  550,\n",
       "  5710,\n",
       "  465,\n",
       "  12036,\n",
       "  11,\n",
       "  6405): 257,\n",
       " (40,\n",
       "  367,\n",
       "  2885,\n",
       "  1464,\n",
       "  1807,\n",
       "  3619,\n",
       "  402,\n",
       "  271,\n",
       "  10899,\n",
       "  2138,\n",
       "  257,\n",
       "  7026,\n",
       "  15632,\n",
       "  438,\n",
       "  2016,\n",
       "  257,\n",
       "  922,\n",
       "  5891,\n",
       "  1576,\n",
       "  438,\n",
       "  568,\n",
       "  340,\n",
       "  373,\n",
       "  645,\n",
       "  1049,\n",
       "  5975,\n",
       "  284,\n",
       "  502,\n",
       "  284,\n",
       "  3285,\n",
       "  326,\n",
       "  11,\n",
       "  287,\n",
       "  262,\n",
       "  6001,\n",
       "  286,\n",
       "  465,\n",
       "  13476,\n",
       "  11,\n",
       "  339,\n",
       "  550,\n",
       "  5710,\n",
       "  465,\n",
       "  12036,\n",
       "  11,\n",
       "  6405,\n",
       "  257): 5527,\n",
       " (40,\n",
       "  367,\n",
       "  2885,\n",
       "  1464,\n",
       "  1807,\n",
       "  3619,\n",
       "  402,\n",
       "  271,\n",
       "  10899,\n",
       "  2138,\n",
       "  257,\n",
       "  7026,\n",
       "  15632,\n",
       "  438,\n",
       "  2016,\n",
       "  257,\n",
       "  922,\n",
       "  5891,\n",
       "  1576,\n",
       "  438,\n",
       "  568,\n",
       "  340,\n",
       "  373,\n",
       "  645,\n",
       "  1049,\n",
       "  5975,\n",
       "  284,\n",
       "  502,\n",
       "  284,\n",
       "  3285,\n",
       "  326,\n",
       "  11,\n",
       "  287,\n",
       "  262,\n",
       "  6001,\n",
       "  286,\n",
       "  465,\n",
       "  13476,\n",
       "  11,\n",
       "  339,\n",
       "  550,\n",
       "  5710,\n",
       "  465,\n",
       "  12036,\n",
       "  11,\n",
       "  6405,\n",
       "  257,\n",
       "  5527): 27075,\n",
       " (40,\n",
       "  367,\n",
       "  2885,\n",
       "  1464,\n",
       "  1807,\n",
       "  3619,\n",
       "  402,\n",
       "  271,\n",
       "  10899,\n",
       "  2138,\n",
       "  257,\n",
       "  7026,\n",
       "  15632,\n",
       "  438,\n",
       "  2016,\n",
       "  257,\n",
       "  922,\n",
       "  5891,\n",
       "  1576,\n",
       "  438,\n",
       "  568,\n",
       "  340,\n",
       "  373,\n",
       "  645,\n",
       "  1049,\n",
       "  5975,\n",
       "  284,\n",
       "  502,\n",
       "  284,\n",
       "  3285,\n",
       "  326,\n",
       "  11,\n",
       "  287,\n",
       "  262,\n",
       "  6001,\n",
       "  286,\n",
       "  465,\n",
       "  13476,\n",
       "  11,\n",
       "  339,\n",
       "  550,\n",
       "  5710,\n",
       "  465,\n",
       "  12036,\n",
       "  11,\n",
       "  6405,\n",
       "  257,\n",
       "  5527,\n",
       "  27075): 11}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_target_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b19566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ---> I\n",
      "I --->  H\n",
      "I H ---> AD\n",
      "I HAD --->  always\n",
      "I HAD always --->  thought\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "for i in range(context_size + 1):\n",
    "  context = sample[:i]\n",
    "  target = sample[i]\n",
    "  print(tokenizer.decode(context), \"--->\", tokenizer.decode([target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9d46a4",
   "metadata": {},
   "source": [
    "## We must build our own DataLoader\n",
    "\n",
    "We build x and y tensors to represent the input-target pairs above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1c7f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class GPTDataset(Dataset):\n",
    "  def __init__(self, txt, tokenizer, max_length, stride):\n",
    "    self.input_ids = []\n",
    "    self.target_ids = []\n",
    "\n",
    "    token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "    for i in range(0, len(token_ids) - max_length, stride):\n",
    "      input_chunk = token_ids[i:i + max_length]\n",
    "      target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "      self.input_ids.append(torch.tensor(input_chunk))\n",
    "      self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.input_ids)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8422a411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(txt, batch_size=4, max_length=256,\n",
    "                      stride=128, shuffle=True, drop_last=True,\n",
    "                      num_workers=0):\n",
    "  # batch size is the number of items per batch the model processes at a time BEFORE updating its parameters\n",
    "  # num_workers is used for parallel processing\n",
    "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "  dataset = GPTDataset(txt, tokenizer, max_length, stride)\n",
    "  \n",
    "  dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=shuffle,\n",
    "    drop_last=drop_last,\n",
    "    num_workers=num_workers\n",
    "  )\n",
    "\n",
    "  return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "881b9a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.5.1\n",
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "  raw_text = f.read()\n",
    "\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "dataloader = create_dataloader(raw_text, 1, 4, 1, False)\n",
    "\n",
    "data_iter = iter(dataloader) # converts an iterable object (like a list, tuple, or string) into an iterator\n",
    "first_batch = next(data_iter) # retrieves the next item from that iterator\n",
    "print(first_batch)\n",
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c66eda2",
   "metadata": {},
   "source": [
    "max_length is 4, so each batch has 4 tokens\n",
    "\n",
    "stride is 1, so this sliding window moves by 1 each time\n",
    "\n",
    "> small batch sizes use less memory during training but take a while to train (because store fewer activations at once)\n",
    "\n",
    "> large batch sizes use more memory but require fewer parameter updates (faster because only perform update for aggregated batch)\n",
    "\n",
    "Recall we do\n",
    "$$\n",
    "\\nabla \\mathcal{L}(\\theta) = \\frac{1}{B} \\sum_{i=1}^B \\nabla\\ell(x_i, \\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdac608",
   "metadata": {},
   "source": [
    "__Changing stride to 2__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "61b32115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
      "[tensor([[2885, 1464, 1807, 3619]]), tensor([[1464, 1807, 3619,  402]])]\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader(raw_text, 1, 4, 2, False)\n",
    "\n",
    "data_iter = iter(dataloader) # converts an iterable object (like a list, tuple, or string) into an iterator\n",
    "first_batch = next(data_iter) # retrieves the next item from that iterator\n",
    "print(first_batch)\n",
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612ff345",
   "metadata": {},
   "source": [
    "__Demonstration adjusting batch size below!__\n",
    "\n",
    "We can see each tensor for the batch has a collection of 8 individual sequences and targets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a4523678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]]), tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])]\n",
      "[tensor([[  287,   262,  6001,   286],\n",
      "        [  465, 13476,    11,   339],\n",
      "        [  550,  5710,   465, 12036],\n",
      "        [   11,  6405,   257,  5527],\n",
      "        [27075,    11,   290,  4920],\n",
      "        [ 2241,   287,   257,  4489],\n",
      "        [   64,   319,   262, 34686],\n",
      "        [41976,    13,   357, 10915]]), tensor([[  262,  6001,   286,   465],\n",
      "        [13476,    11,   339,   550],\n",
      "        [ 5710,   465, 12036,    11],\n",
      "        [ 6405,   257,  5527, 27075],\n",
      "        [   11,   290,  4920,  2241],\n",
      "        [  287,   257,  4489,    64],\n",
      "        [  319,   262, 34686, 41976],\n",
      "        [   13,   357, 10915,   314]])]\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader(raw_text, 8, 4, 4, False)\n",
    "\n",
    "data_iter = iter(dataloader) # converts an iterable object (like a list, tuple, or string) into an iterator\n",
    "first_batch = next(data_iter) # retrieves the next item from that iterator\n",
    "print(first_batch)\n",
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609cba32",
   "metadata": {},
   "source": [
    "# Lecture 10: Token Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fd57a9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([5,2,1,3]) # example token IDs\n",
    "\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim) # a lookup table that stores fixed-size vector representations (embeddings) of a dictionary or vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e1415dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.3374, -0.1778, -0.1690],\n",
       "        [ 0.9178,  1.5810,  1.3010],\n",
       "        [ 1.2753, -0.2010, -0.1606],\n",
       "        [-0.4015,  0.9666, -1.1481],\n",
       "        [-1.1589,  0.3255, -0.6315],\n",
       "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer.weight # we optimize these weights during training\n",
    "\n",
    "# each row is a vector embedding of each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1ca01823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# access the embedding for token with ID 3... a LOOKUP TABLE\n",
    "# this syntax tells our embedding layer/lookup table to look at row 3\n",
    "embedding_layer(torch.tensor([3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1a05d8f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.8400, -0.7849, -1.4096],\n",
       "        [ 1.2753, -0.2010, -0.1606],\n",
       "        [ 0.9178,  1.5810,  1.3010],\n",
       "        [-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer(input_ids)  # get these specified tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c0c1b0",
   "metadata": {},
   "source": [
    "__The dimensions work now and this is correct. We will actually train these parameters later.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06754d21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
